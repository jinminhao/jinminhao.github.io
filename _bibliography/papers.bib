
@inproceedings{10.1145/3544216.3544251,
author = {Yin, Yucheng and Lin, Zinan and Jin, Minhao and Fanti, Giulia and Sekar, Vyas},
title = {Practical GAN-Based Synthetic IP Header Trace Generation Using NetShare},
year = {2022},
isbn = {9781450394208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544216.3544251},
doi = {10.1145/3544216.3544251},
abstract = {We explore the feasibility of using Generative Adversarial Networks (GANs) to automatically learn generative models to generate synthetic packet- and flow header traces for networking tasks (e.g., telemetry, anomaly detection, provisioning). We identify key fidelity, scalability, and privacy challenges and tradeoffs in existing GAN-based approaches. By synthesizing domain-specific insights with recent advances in machine learning and privacy, we identify design choices to tackle these challenges. Building on these insights, we develop an end-to-end framework, NetShare. We evaluate NetShare on six diverse packet header traces and find that: (1) across all distributional metrics and traces, it achieves 46\% more accuracy than baselines and (2) it meets users' requirements of downstream tasks in evaluating accuracy and rank ordering of candidate approaches.},
booktitle = {Proceedings of the ACM SIGCOMM 2022 Conference},
pages = {458-472},
numpages = {15},
keywords = {privacy, generative adversarial networks, network packets, synthetic data generation, network flows},
location = {Amsterdam, Netherlands},
series = {SIGCOMM '22},
code = {https://github.com/netsharecmu/NetShare},
pdf = {https://dl.acm.org/doi/pdf/10.1145/3544216.3544251},
abbr = {SIGCOMM},
selected={true}
}

@inproceedings{pcapshare,
author = {Yin, Yucheng and Lin, Zinan and Jin, Minhao and Fanti, Giulia and Sekar, Vyas},
title = {PcapShare: Exploring the Feasibility of GANs for Synthetic Packet Header Trace Generation},
year = {2022},
booktitle = {Fourteenth InternationalConference on COMmunication Systems and NETworkS (demo)},
series = {COMSNETS '22},
abbr = {COMSNETS},
}

@article{Wang2021,
author = {Wang, Tianyu and Jin, Minhao and Li, Mian},
title = {Towards accurate and interpretable surgical skill assessment: a video-based method for skill score prediction and guiding feedback generation},
journal = {International Journal of Computer Assisted Radiology and Surgery},
year = {2021},
month = {Sep},
day = {01},
volume = {16},
number = {9},
pages = {1595-1605},
abstract = {Recently, automatic surgical skill assessment has received the attention given the increasingly important role of surgical training. The assessment usually involves skill score prediction and further feedback generation. Existing work on skill score prediction is limited with several challenges and deserves more promising outcomes. For the feedback, most work identifies the flaws on the granularity of video frames or clips. It thus remains to be explored how to identify poorly performed gestures (segments) and further how to provide good references for improvement.},
issn = {1861-6429},
doi = {10.1007/s11548-021-02448-4},
url = {https://doi.org/10.1007/s11548-021-02448-4},
abbr = {IJCARS}
}

@inproceedings{10.1145/3341105.3374092,
author = {Wang, Tianyu and Jin, Minhao and Wang, Jingying and Wang, Yijie and Li, Mian},
title = {Towards a Data-Driven Method for RGB Video-Based Hand Action Quality Assessment in Real Time},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374092},
doi = {10.1145/3341105.3374092},
abstract = {In recent years, the research community has begun to explore Video-Based Action Quality Assessment on Human Body (VB-AQA), while few work focuses on Video-Based Action Quality Assessment on Human Hand (VH-AQA) yet. The current work on VB-AQA fails to deal with the inconsistency between captured features and the reality due to the changing angles of the camera, leaving a huge gap between VB-AQA and VH-AQA, while the computational efficiency is another critical problem. In this paper, a novel data-driven method for real-time VH-AQA is proposed. Features are formulated as spatio-temporal hand poses in this method and extracted via four steps: hand segmentation, 2D hand pose estimation, 3D hand pose estimation and hand pose organization. Based on the extracted features an assessment model is applied to evaluate the performance of actions and indicate the most promising adjustment as the feedback. We demonstrate the evaluation accuracy and computational efficiency of our method using our own Origami Video Dataset. For the latter, two new metrics are designed. It turns out that our method provides opportunities for real-time digital reconstruction of physical world activities and timely assessment.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {2117-2120},
numpages = {4},
keywords = {video-based action quality assessment on human hand, data-driven, origami dataset, real time, hand pose organization},
location = {Brno, Czech Republic},
series = {SAC '20},
pdf = {https://dl.acm.org/doi/pdf/10.1145/3341105.3374092},
abbr = {SAC}
}